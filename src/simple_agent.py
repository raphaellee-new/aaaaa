# -*- coding: utf-8 -*-
# Auto-generated from Simple_Agent.ipynb (code cells only)
# NOTE: Review and remove any Colab-only imports/usages if present (google.colab.*).

# ------------------------------
# Cell 1
# ------------------------------
SERVICE_KEY = "FqDrDXAFVLM5UoSYzzyC619C0HzxHTDx7a3Ot5Q0M90m62wl700+7jt0PTBpFhP1qivmuO5jVLgmiJHuPlPkNA=="
BASE_URL = "https://apis.data.go.kr/1371000/policyNewsService/policyNewsList"

# ------------------------------
# Cell 2
# ------------------------------
# ============================================================
# Korea.kr ì •ì±…ë‰´ìŠ¤(ì •ì±…ë¸Œë¦¬í•‘) ìˆ˜ì§‘ -> DataFrame -> CSV ì €ì¥/ë‹¤ìš´ë¡œë“œ
# Google Colab "í•œ ì…€" ì‹¤í–‰ìš©
# ============================================================


import re
import requests
import pandas as pd
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import quote
from datetime import datetime, timedelta
from google.colab import files

API_URL = "https://apis.data.go.kr/1371000/policyNewsService/policyNewsList"

# ----------------------------
# 0) ìœ í‹¸
# ----------------------------
def normalize_service_key(service_key: str) -> str:
    """
    - data.go.krì—ì„œ serviceKeyê°€ 'Encoding(%)' í˜•íƒœë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - 'Decoding(+ / = í¬í•¨)' í˜•íƒœë©´ URL-safeë¡œ ì¸ì½”ë”©
    """
    k = service_key.strip()
    return k if "%" in k else quote(k, safe="")

def mask_url(u: str) -> str:
    return re.sub(r"(serviceKey=)[^&]+", r"\1***", u)

def strip_html(html: str) -> str:
    if not html:
        return ""
    soup = BeautifulSoup(html, "lxml")
    text = soup.get_text("\n")
    return re.sub(r"\n{3,}", "\n\n", text).strip()

def strip_namespaces(root: ET.Element) -> ET.Element:
    """
    ElementTree íƒœê·¸ì— {namespace} ê°€ ë¶™ëŠ” ê²½ìš° ì œê±°
    """
    for el in root.iter():
        if isinstance(el.tag, str) and el.tag.startswith("{"):
            el.tag = el.tag.split("}", 1)[1]
    return root

def daterange_chunks(start_yyyymmdd: str, end_yyyymmdd: str, chunk_days: int = 3):
    """
    APIê°€ 3ì¼ ì´ˆê³¼ ì¡°íšŒ ì‹œ THREE_DAYS_OVER_ERRORë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ,
    inclusive ë‚ ì§œ êµ¬ê°„ì„ ìµœëŒ€ 3ì¼ ë‹¨ìœ„ë¡œ ìª¼ê°œ í˜¸ì¶œí•œë‹¤.
    """
    start = datetime.strptime(start_yyyymmdd, "%Y%m%d").date()
    end   = datetime.strptime(end_yyyymmdd, "%Y%m%d").date()
    cur = start
    while cur <= end:
        chunk_end = min(cur + timedelta(days=chunk_days - 1), end)
        yield cur.strftime("%Y%m%d"), chunk_end.strftime("%Y%m%d")
        cur = chunk_end + timedelta(days=1)

# ----------------------------
# 1) API í˜¸ì¶œ + íŒŒì‹±
# ----------------------------
def parse_items_from_xml(xml_text: str, debug: bool = False):
    root = ET.fromstring(xml_text)

    # resultCode/resultMsg
    result_code = (root.findtext(".//resultCode") or "").strip()
    result_msg  = (root.findtext(".//resultMsg")  or "").strip()

    # namespace ì œê±° í›„ item íƒìƒ‰
    root = strip_namespaces(root)

    items = root.findall(".//item")
    # í˜¹ì‹œ item íƒœê·¸ëª…ì´ ë‹¤ë¥´ê²Œ ëë‚˜ëŠ” ê²½ìš° ëŒ€ë¹„(í¬ê·€ ì¼€ì´ìŠ¤)
    if len(items) == 0:
        items = [el for el in root.iter() if isinstance(el.tag, str) and el.tag.lower().endswith("item")]

    if debug:
        print("  [PARSE] resultCode:", result_code, "resultMsg:", result_msg, "items:", len(items))

    rows = []
    for it in items:
        def t(tag):
            x = it.find(tag)
            return x.text.strip() if (x is not None and x.text) else ""

        contents_type = t("ContentsType")
        data_contents = t("DataContents")

        rows.append({
            "NewsItemId": t("NewsItemId"),
            "GroupingCode": t("GroupingCode"),
            "MinisterCode": t("MinisterCode"),
            "ApproveDate": t("ApproveDate"),
            "ModifyDate": t("ModifyDate"),
            "Title": t("Title"),
            "SubTitle1": t("SubTitle1"),
            "SubTitle2": t("SubTitle2"),
            "SubTitle3": t("SubTitle3"),
            "ContentsType": contents_type,
            "OriginalUrl": t("OriginalUrl"),
            "ThumbnailUrl": t("ThumbnailUrl"),
            "OriginalimgUrl": t("OriginalimgUrl"),
            "DataContents_text": strip_html(data_contents) if contents_type.upper() == "H" else data_contents,
            "DataContents_raw": data_contents,
        })

    meta = {"resultCode": result_code, "resultMsg": result_msg}
    return rows, meta

def call_api(service_key: str, start_date: str, end_date: str, timeout: int = 30, debug: bool = True):
    sk = normalize_service_key(service_key)
    url = f"{API_URL}?serviceKey={sk}&startDate={start_date}&endDate={end_date}"

    resp = requests.get(url, timeout=timeout)
    resp.raise_for_status()

    if debug:
        print("[CALL]", start_date, "~", end_date,
              "|", resp.status_code,
              "|", mask_url(resp.url),
              "| len:", len(resp.text))

    rows, meta = parse_items_from_xml(resp.text, debug=False)

    # resultCodeê°€ 0ì´ ì•„ë‹ˆë©´ ì—ëŸ¬ë¡œ ì·¨ê¸‰
    if meta.get("resultCode") != "0":
        if debug:
            print("  -> API ERROR:", meta.get("resultCode"), meta.get("resultMsg"))
            print("  -> response head:", resp.text[:250])
        return [], meta, resp.text

    return rows, meta, resp.text

def fetch_policy_news_range(service_key: str, start_date: str, end_date: str, debug: bool = True):
    all_rows = []
    errors = []

    for s, e in daterange_chunks(start_date, end_date, chunk_days=3):
        rows, meta, _ = call_api(service_key, s, e, debug=debug)
        if meta.get("resultCode") == "0":
            for r in rows:
                r["req_startDate"] = s
                r["req_endDate"] = e
            all_rows.extend(rows)
        else:
            errors.append({"startDate": s, "endDate": e, **meta})

    df = pd.DataFrame(all_rows)
    err_df = pd.DataFrame(errors)

    # ì¤‘ë³µ ì œê±°
    if not df.empty and "NewsItemId" in df.columns:
        df = df.drop_duplicates(subset=["NewsItemId"]).reset_index(drop=True)

    # ì½˜í…ì¸  ê¸¸ì´(ìš”ì•½/í•„í„°ë§ìš©)
    if not df.empty and "DataContents_text" in df.columns:
        df["content_length"] = df["DataContents_text"].astype(str).str.len()

    return df, err_df

# ----------------------------
# 2) ì‹¤í–‰ íŒŒë¼ë¯¸í„°
# ----------------------------
# data.go.krì—ì„œ ë°œê¸‰ë°›ì€ serviceKey ì…ë ¥

# ë‚ ì§œ ë²”ìœ„: YYYYMMDD
# - ê¸°ë³¸ì€ "ìµœê·¼ 3ì¼"ë¡œ ì„¤ì •(ì—ëŸ¬ ì—†ì´ ë™ì‘ í™•ì¸ ìš©ì´)
today = datetime.now().date()
START_DATE = (today - timedelta(days=2)).strftime("%Y%m%d")
END_DATE   = today.strftime("%Y%m%d")

# ì‚¬ìš©ìê°€ ì§ì ‘ ì§€ì •í•˜ê³  ì‹¶ìœ¼ë©´ ìœ„ ë‘ ì¤„ ëŒ€ì‹  ì•„ë˜ì²˜ëŸ¼ ë°”ê¾¸ì„¸ìš”:
# START_DATE = "20251215"
# END_DATE   = "20251222"

print("=== RUN PARAMS ===")
print("START_DATE:", START_DATE, "END_DATE:", END_DATE)

# ----------------------------
# 3) ìˆ˜ì§‘ ì‹¤í–‰
# ----------------------------
df, err_df = fetch_policy_news_range(SERVICE_KEY, START_DATE, END_DATE, debug=True)

print("\n=== RESULT SUMMARY ===")
print("rows:", len(df))
print("errors:", len(err_df))
if not err_df.empty:
    display(err_df)

display(df.head(5) if not df.empty else pd.DataFrame([{"status": "no rows"}]))

# ----------------------------
# 4) CSV ì €ì¥ + ë‹¤ìš´ë¡œë“œ
# ----------------------------
# ì»¬ëŸ¼ ì •ë ¬(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ)
ordered_cols = [
    "NewsItemId", "ApproveDate", "ModifyDate", "Title",
    "SubTitle1", "SubTitle2", "SubTitle3",
    "GroupingCode", "MinisterCode",
    "OriginalUrl", "ContentsType",
    "content_length", "DataContents_text",
    "req_startDate", "req_endDate",
]
if not df.empty:
    ordered_cols = [c for c in ordered_cols if c in df.columns]
    df_out = df[ordered_cols].copy()
else:
    df_out = df.copy()

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
csv_path = f"policy_news_{START_DATE}_{END_DATE}_{timestamp}.csv"

df_out.to_csv(csv_path, index=False, encoding="utf-8-sig")
print("\nSaved CSV:", csv_path)

# Colab ë‹¤ìš´ë¡œë“œ
files.download(csv_path)

# ------------------------------
# Cell 3
# ------------------------------
import os
os.environ["MISTRAL_API_KEY"] = "sLWOHYqD1V63JCo5MvL3jbKJkEjGieO6"

# ------------------------------
# Cell 4
# ------------------------------
# =========================
# Google Colab One-Cell Script
# CSV (ê¸°ë³¸ê²½ë¡œ) -> Mistral AI -> 2ì¤„ ìš”ì•½ + í‚¤ì›Œë“œ -> ê²°ê³¼ CSV
# =========================


import os
import json
import time
import re
import pandas as pd
from tqdm import tqdm
from getpass import getpass

from mistralai import Mistral

# -------------------------------------------------
# 0. íŒŒì¼ ê²½ë¡œ ë° ì»¬ëŸ¼ ì„¤ì • (Colab ê¸°ë³¸ ê²½ë¡œ)
# -------------------------------------------------
INPUT_CSV = "/content/policy_news_20251220_20251222_20251222_192215.csv"
OUTPUT_CSV = "/content/policy_news_with_summary_keywords.csv"

TEXT_COL = "DataContents_text"   # ë³¸ë¬¸
TITLE_COL = "Title"              # ì œëª©
SUB1_COL = "SubTitle1"            # ë¶€ì œ (ì—†ìœ¼ë©´ ìë™ ë¬´ì‹œ)

MODEL = "mistral-small-latest"
MAX_CONTENT_CHARS = 4500
SLEEP_SEC = 0.3
MAX_RETRIES = 5

# -------------------------------------------------
# 1. Mistral API Key ì„¤ì •
# -------------------------------------------------
api_key = os.environ.get("MISTRAL_API_KEY")
if not api_key:
    api_key = getpass("MISTRAL_API_KEY ì…ë ¥ (ì…ë ¥ê°’ ë¹„í‘œì‹œ): ").strip()

client = Mistral(api_key=api_key)

# -------------------------------------------------
# 2. ìœ í‹¸ í•¨ìˆ˜
# -------------------------------------------------
def truncate_text(text, max_chars):
    if pd.isna(text):
        return ""
    text = str(text)
    return text if len(text) <= max_chars else text[:max_chars] + "\n...(truncated)"

def safe_json_parse(text):
    try:
        return json.loads(text)
    except Exception:
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except Exception:
                return None
    return None

def mistral_summarize(title, subtitle, content):
    prompt = f"""
[ì œëª©] {title}
[ë¶€ì œ] {subtitle}

[ë³¸ë¬¸]
{truncate_text(content, MAX_CONTENT_CHARS)}

ìš”ì²­:
- í•œêµ­ì–´ë¡œ 2ì¤„ ìš”ì•½
- í•µì‹¬ í‚¤ì›Œë“œ 5~10ê°œ
- ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ

í˜•ì‹:
{{
  "summary_line1": "...",
  "summary_line2": "...",
  "keywords": ["...", "..."]
}}
""".strip()

    for attempt in range(MAX_RETRIES):
        try:
            response = client.chat.complete(
                model=MODEL,
                messages=[
                    {"role": "system", "content": "You are a Korean policy/news analyst. Output JSON only."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.2,
            )

            content = response.choices[0].message.content
            data = safe_json_parse(content)
            if not data:
                raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨")

            return {
                "summary_line1": data.get("summary_line1", ""),
                "summary_line2": data.get("summary_line2", ""),
                "keywords": ", ".join(dict.fromkeys(data.get("keywords", [])))
            }

        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                return {
                    "summary_line1": "",
                    "summary_line2": "",
                    "keywords": "",
                    "error": str(e)
                }
            time.sleep(2 ** attempt)

# -------------------------------------------------
# 3. CSV ë¡œë“œ
# -------------------------------------------------
df = pd.read_csv(INPUT_CSV)

if TEXT_COL not in df.columns:
    raise ValueError(f"ë³¸ë¬¸ ì»¬ëŸ¼ '{TEXT_COL}' ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

# -------------------------------------------------
# 4. ë ˆì½”ë“œë³„ ìš”ì•½ + í‚¤ì›Œë“œ ìƒì„±
# -------------------------------------------------
summary1, summary2, keywords, errors = [], [], [], []

for _, row in tqdm(df.iterrows(), total=len(df), desc="Mistral ì²˜ë¦¬ ì¤‘"):
    result = mistral_summarize(
        row.get(TITLE_COL, ""),
        row.get(SUB1_COL, ""),
        row.get(TEXT_COL, "")
    )

    summary1.append(result.get("summary_line1", ""))
    summary2.append(result.get("summary_line2", ""))
    keywords.append(result.get("keywords", ""))
    errors.append(result.get("error", ""))

    time.sleep(SLEEP_SEC)

df["summary_line1"] = summary1
df["summary_line2"] = summary2
df["keywords"] = keywords
df["mistral_error"] = errors

# -------------------------------------------------
# 5. ê²°ê³¼ ì €ì¥
# -------------------------------------------------
df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
print(f"ì™„ë£Œ: {OUTPUT_CSV}")

from google.colab import files
files.download(OUTPUT_CSV)

# ------------------------------
# Cell 5
# ------------------------------
# ------------------------------------------------------------
# (Google Colab 1-cell) policy_news_with_summary_keywords.csv
#  -> Mistral AIë¡œ "ë³´ê³ ì„œ ì´ˆì•ˆ(Markdown)" ìƒì„± + ë‹¤ìš´ë¡œë“œ
#  - ê¶Œì¥ ìµœì¢… êµ¬ì„±(ì»¬ëŸ¼ ê³ ì • ë§¤í•‘: Title, DataContents_text, ApproveDate, OriginalUrl, summary_line1/2, keywords)
# ------------------------------------------------------------


import os
import re
import json
import pandas as pd
from datetime import datetime
from getpass import getpass

from mistralai import Mistral

# ========== 0) ì…ë ¥ íŒŒì¼ ìë™ íƒìƒ‰ ==========
CANDIDATE_PATHS = [
    "policy_news_with_summary_keywords.csv",
    "/content/policy_news_with_summary_keywords.csv",
    "/mnt/data/policy_news_with_summary_keywords.csv",
]

INPUT_CSV = next((p for p in CANDIDATE_PATHS if os.path.exists(p)), None)
if not INPUT_CSV:
    raise FileNotFoundError(
        "CSV íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Colabì— ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•˜ê³ , "
        "íŒŒì¼ëª…ì´ ë‹¤ë¥´ë©´ CANDIDATE_PATHSì— ì¶”ê°€í•˜ì„¸ìš”."
    )

# ========== 1) Mistral API Key ì„¤ì • ==========
# ê¶Œì¥: Colab Secrets/í™˜ê²½ë³€ìˆ˜ì— ì €ì¥ (os.environ["MISTRAL_API_KEY"] = "...")
if not os.environ.get("MISTRAL_API_KEY"):
    os.environ["MISTRAL_API_KEY"] = getpass("Mistral API Key ì…ë ¥ (í‘œì‹œë˜ì§€ ì•ŠìŒ): ")

client = Mistral(api_key=os.environ["MISTRAL_API_KEY"])

# ========== 2) CSV ë¡œë“œ ==========
df = pd.read_csv(INPUT_CSV)

# ========== 3) ê¶Œì¥ ìµœì¢… ì»¬ëŸ¼ ë§¤í•‘(ê³ ì •) ==========
required_cols = [
    "Title", "DataContents_text", "ApproveDate", "OriginalUrl",
    "summary_line1", "summary_line2", "keywords"
]
missing = [c for c in required_cols if c not in df.columns]
if missing:
    raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}\ní˜„ì¬ CSV ì»¬ëŸ¼: {list(df.columns)}")

col_title = "Title"
col_body  = "DataContents_text"
col_date  = "ApproveDate"
col_url   = "OriginalUrl"
col_kw    = "keywords"

# 2ì¤„ ìš”ì•½ ê²°í•©
df["summary"] = (
    df["summary_line1"].fillna("").astype(str).str.strip()
    + "\n" +
    df["summary_line2"].fillna("").astype(str).str.strip()
)
col_sum = "summary"

# ê²°ì¸¡/í˜• ë³€í™˜
for c in [col_title, col_body, col_date, col_url, col_kw, col_sum]:
    df[c] = df[c].fillna("").astype(str)

def clean_text(s: str) -> str:
    s = re.sub(r"\s+", " ", str(s)).strip()
    return s

# ========== 4) LLM ì…ë ¥ìš© ì•„ì´í…œ êµ¬ì„± ==========
MAX_BODY_CHARS_PER_ITEM = 1400   # ë³¸ë¬¸ì´ ê¸¸ë©´ í† í° ì´ˆê³¼ ë°©ì§€ ìœ„í•´ ì ˆë‹¨
MAX_ITEMS_FOR_ANALYSIS  = 80     # ë°ì´í„°ê°€ ë§ìœ¼ë©´ 40~60ìœ¼ë¡œ ì¤„ì´ëŠ” ê²ƒì„ ê¶Œì¥

items = []
for i, row in df.iterrows():
    items.append({
        "idx": i + 1,
        "title": clean_text(row[col_title]),
        "date": clean_text(row[col_date]),
        "url": clean_text(row[col_url]),
        "body": clean_text(row[col_body])[:MAX_BODY_CHARS_PER_ITEM],
        "summary": row[col_sum].strip(),
        "keywords": clean_text(row[col_kw]),
        "content_length": len(str(row[col_body])) if "content_length" in df.columns else None,
    })

analysis_items = items[:MAX_ITEMS_FOR_ANALYSIS]

analysis_payload = []
for it in analysis_items:
    analysis_payload.append({
        "idx": it["idx"],
        "title": it["title"],
        "date": it["date"],
        "url": it["url"],
        "summary": it["summary"],
        "keywords": it["keywords"],
        "body": it["body"],
    })

# ========== 5) Mistral í˜¸ì¶œ ìœ í‹¸ ==========
MODEL = "mistral-small-latest"   # í•„ìš” ì‹œ "mistral-medium-latest" ë“±ìœ¼ë¡œ ë³€ê²½
TEMPERATURE = 0.2

def chat(messages, model=MODEL, temperature=TEMPERATURE):
    resp = client.chat.complete(
        model=model,
        messages=messages,
        temperature=temperature,
    )
    return resp.choices[0].message.content

def extract_json(text: str):
    text = (text or "").strip()
    m = re.search(r"(\{.*\})", text, flags=re.DOTALL)
    if not m:
        raise ValueError("ëª¨ë¸ ì¶œë ¥ì—ì„œ JSONì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n--- ëª¨ë¸ ì¶œë ¥(ì•ë¶€ë¶„) ---\n" + text[:1500])
    return json.loads(m.group(1))

# ========== 6) (1ë‹¨ê³„) ì´ìŠˆ êµ¬ì¡°í™” JSON ìƒì„± ==========
system_1 = (
    "ë‹¹ì‹ ì€ ê³µê³µê¸°ê´€ ë³´ê³ ì„œ ì‘ì„± ë³´ì¡°ìì…ë‹ˆë‹¤. "
    "ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ì´ìŠˆë¥¼ êµ¬ì¡°í™”í•´ ì£¼ì„¸ìš”. "
    "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”(ì½”ë“œë¸”ë¡/ì„¤ëª… ê¸ˆì§€)."
)

user_1 = f"""
[ìš”ì²­]
ë‹¤ìŒ ë‰´ìŠ¤ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬, ë³´ê³ ì„œ ì‘ì„±ì— ì‚¬ìš©í•  'ì´ìŠˆ êµ¬ì¡°í™” ê²°ê³¼'ë¥¼ JSONìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.

[ê·œì¹™]
- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥
- top_issues: ìƒìœ„ 5~8ê°œ ì´ìŠˆ
  - ê° ì´ìŠˆ ê°ì²´ í•„ë“œ:
    - issue: ì´ìŠˆëª…(ì§§ê²Œ)
    - situation: í˜„í™© ìš”ì•½(2~4ë¬¸ì¥)
    - why_it_matters: ì™œ ì¤‘ìš”í•œê°€(ê³µê³µê¸°ê´€ ê´€ì )
    - evidence_items: ê·¼ê±° ë‰´ìŠ¤ idx ë°°ì—´(2~8ê°œ)
    - risk_or_opportunity: ë¦¬ìŠ¤í¬/ê¸°íšŒ ìš”ì•½
    - suggested_actions: ê¶Œê³  ì¡°ì¹˜ ë¶ˆë¦¿ 3~6ê°œ(ë¬¸ì¥í˜•)
- ê°€ëŠ¥í•˜ë©´ keywords/summaryë¥¼ ìš°ì„  í™œìš©í•˜ê³ , ë¶€ì¡±í•  ë•Œ bodyë¥¼ ì°¸ê³ 

[ë‰´ìŠ¤ ëª©ë¡ JSON]
{json.dumps(analysis_payload, ensure_ascii=False)}
"""

issue_json_text = chat([
    {"role": "system", "content": system_1},
    {"role": "user", "content": user_1},
])

issue_struct = extract_json(issue_json_text)

# ========== 7) (2ë‹¨ê³„) ìµœì¢… ë³´ê³ ì„œ(Markdown) ìƒì„± ==========
# ì¶œì²˜ ëª©ë¡ ìƒì„± (idx-title-date-url)
source_lines = []
for it in analysis_items:
    d = f" ({it['date']})" if it["date"] else ""
    u = f" | {it['url']}" if it["url"] else ""
    source_lines.append(f"- [{it['idx']}] {it['title']}{d}{u}")

system_2 = (
    "ë‹¹ì‹ ì€ ê³µê³µê¸°ê´€ ë³´ê³ ì„œ ì‘ì„±ìì…ë‹ˆë‹¤. "
    "ì£¼ì–´ì§„ ì´ìŠˆ êµ¬ì¡°(JSON)ì™€ ì¶œì²˜ ëª©ë¡ì„ ê·¼ê±°ë¡œ í•œêµ­ì–´ Markdown ë³´ê³ ì„œ ì´ˆì•ˆì„ ì‘ì„±í•˜ì„¸ìš”."
)

user_2 = f"""
[ìš”ì²­]
ì•„ë˜ ì´ìŠˆ êµ¬ì¡°(JSON)ì™€ ì¶œì²˜ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ 'ë³´ê³ ì„œ ì´ˆì•ˆ'ì„ Markdownìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ë³´ê³ ì„œ í˜•ì‹]
# ì •ì±…ë‰´ìŠ¤ ë¶„ì„ ë³´ê³ ì„œ ì´ˆì•ˆ (ê¸°ê°„/í‚¤ì›Œë“œëŠ” ìƒí™©ì— ë§ê²Œ ì‘ì„±)
## ëª©ì 
- 3~5ì¤„

## ê°œìš”
- ë¶ˆë¦¿ 5~8ê°œ (í•µì‹¬ ì´ìŠˆ ìš”ì•½)

## ë³¸ë¬¸
- ì´ìŠˆë³„ë¡œ ### ì†Œì œëª© êµ¬ì„±
- ê° ì´ìŠˆë§ˆë‹¤ ì•„ë˜ ì†Œêµ¬ì¡°ë¥¼ í¬í•¨:
  - **í˜„í™© ìš”ì•½**: 2~4ë¬¸ì¥
  - **ê³µê³µê¸°ê´€ ì‹œì‚¬ì /ë¦¬ìŠ¤í¬**: ë¶ˆë¦¿ 2~4ê°œ
  - **ê¶Œê³  ì¡°ì¹˜**: ë¶ˆë¦¿ 2~5ê°œ
  - **ê·¼ê±° ë‰´ìŠ¤**: [idx] í˜•íƒœë¡œ 2~6ê°œ ì¸ìš©

## ì¶œì²˜
- ì•„ë˜ ì¶œì²˜ ëª©ë¡ì„ ê·¸ëŒ€ë¡œ ë¶™ì—¬ ë„£ê¸°

[ì´ìŠˆ êµ¬ì¡° JSON]
{json.dumps(issue_struct, ensure_ascii=False)}

[ì¶œì²˜ ëª©ë¡]
{chr(10).join(source_lines)}
"""

report_md = chat([
    {"role": "system", "content": system_2},
    {"role": "user", "content": user_2},
])

# ========== 8) ì €ì¥ + ë‹¤ìš´ë¡œë“œ ==========
out_name = f"report_draft_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
with open(out_name, "w", encoding="utf-8") as f:
    f.write(report_md)

print("=== ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ ===")
print("ì…ë ¥ CSV:", INPUT_CSV)
print("ì €ì¥ íŒŒì¼:", out_name)

# Colab ë‹¤ìš´ë¡œë“œ
from google.colab import files
files.download(out_name)

# ------------------------------
# Cell 6
# ------------------------------
# =========================================================
# Google Colab ë‹¨ì¼ ì…€
# Secrets(userdata.get) -> Markdown -> HTML -> Gmail ë°œì†¡
# =========================================================

# 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

# 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os
import re
import smtplib
from datetime import datetime
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

import markdown as md
from google.colab import userdata

# =========================================================
# 3. Colab Secrets ë¡œë“œ (userdata.get ë°©ì‹)
# =========================================================
GMAIL_ADDRESS = userdata.get("GMAIL_ADDRESS")
TO_EMAIL = userdata.get("TO_EMAIL")

# ì •ìƒ í‚¤ + ì˜¤íƒ€ í‚¤ fallback
GMAIL_APP_PASSWORD = (
    userdata.get("GMAIL_APP_PASSWORD")
    or userdata.get("GMAIL_APP_PASSWORR")
)

missing = []
if not GMAIL_ADDRESS:
    missing.append("GMAIL_ADDRESS")
if not TO_EMAIL:
    missing.append("TO_EMAIL")
if not GMAIL_APP_PASSWORD:
    missing.append("GMAIL_APP_PASSWORD (ë˜ëŠ” GMAIL_APP_PASSWORR)")

if missing:
    raise RuntimeError(
        f"Colab ë³´ì•ˆ ë¹„ë°€(Secrets)ì— ê°’ì´ ì—†ìŠµë‹ˆë‹¤: {missing}\n"
        f"ğŸ”’ ì¢Œì¸¡ Secrets ë©”ë‰´ì—ì„œ ì¶”ê°€/ìˆ˜ì • í›„ ëŸ°íƒ€ì„ ì¬ì‹œì‘í•˜ì„¸ìš”."
    )

print("âœ” Secrets ë¡œë”© ì™„ë£Œ")
print("  - From:", GMAIL_ADDRESS)
print("  - To  :", TO_EMAIL)
print("  - App Password: OK")

# =========================================================
# 4. Markdown íŒŒì¼ ê²½ë¡œ (Colab ê¸°ë³¸ ê²½ë¡œ)
# =========================================================
MD_PATH = "/content/report_draft_20251222_193913.md"

if not os.path.exists(MD_PATH):
    md_files = [f for f in os.listdir("/content") if f.lower().endswith(".md")]
    raise FileNotFoundError(
        f"Markdown íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MD_PATH}\n"
        f"/content ì— ì¡´ì¬í•˜ëŠ” md íŒŒì¼: {md_files}"
    )

# =========================================================
# 5. Markdown íŒŒì¼ ì½ê¸°
# =========================================================
with open(MD_PATH, "r", encoding="utf-8") as f:
    md_text = f.read()

# =========================================================
# 6. Markdown -> HTML ë³€í™˜ (í¬ë§· ìœ ì§€)
# =========================================================
html_body = md.markdown(
    md_text,
    extensions=[
        "extra",
        "tables",
        "fenced_code",
        "sane_lists",
        "toc"
    ]
)

html_template = f"""
<!doctype html>
<html>
<head>
<meta charset="utf-8">
</head>
<body style="font-family: Arial, Helvetica, sans-serif; line-height:1.6; color:#111;">
  <div style="max-width:900px; margin:0 auto; padding:8px 4px;">
    {html_body}
    <hr style="margin-top:24px; border:none; border-top:1px solid #ddd;">
    <div style="font-size:12px; color:#666;">
      <div>ë°œì†¡ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
      <div>ì›ë³¸ íŒŒì¼: {MD_PATH}</div>
    </div>
  </div>
</body>
</html>
"""

plain_text = re.sub(r"\n{3,}", "\n\n", md_text).strip()

# =========================================================
# 7. ë©”ì¼ MIME êµ¬ì„±
# =========================================================
SUBJECT = f"ì •ì±…ë‰´ìŠ¤ ë¶„ì„ ë³´ê³ ì„œ ì´ˆì•ˆ ({datetime.now().strftime('%Y-%m-%d')})"

msg = MIMEMultipart("alternative")
msg["Subject"] = SUBJECT
msg["From"] = GMAIL_ADDRESS
msg["To"] = TO_EMAIL

msg.attach(MIMEText(plain_text, "plain", "utf-8"))
msg.attach(MIMEText(html_template, "html", "utf-8"))

# =========================================================
# 8. Gmail SMTP ë°œì†¡
# =========================================================
def send_gmail(from_addr, app_password, to_addrs, mime_msg):
    recipients = [x.strip() for x in re.split(r"[;,]", to_addrs) if x.strip()]
    if not recipients:
        raise ValueError("ìˆ˜ì‹ ì ì£¼ì†Œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
        server.login(from_addr, app_password)
        server.sendmail(from_addr, recipients, mime_msg.as_string())

send_gmail(GMAIL_ADDRESS, GMAIL_APP_PASSWORD, TO_EMAIL, msg)

print("âœ… ë©”ì¼ ë°œì†¡ ì™„ë£Œ")
print("  - Subject:", SUBJECT)

# ------------------------------
# Cell 7
# ------------------------------
# ============================================================
# DataContents_text â†’ QA Retrieval Embedding (Google Colab)
# ============================================================

# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ)

# ------------------------------------------------------------
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
# ------------------------------------------------------------
import pandas as pd
import numpy as np
import json
import faiss
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# ------------------------------------------------------------
# 2. ì„¤ì •
# ------------------------------------------------------------
INPUT_CSV = "/content/policy_news_with_summary_keywords.csv"

OUTPUT_META_CSV = "/content/policy_news_chunks_meta.csv"
OUTPUT_EMB_NPY  = "/content/policy_news_chunks_embeddings.npy"
OUTPUT_FAISS    = "/content/policy_news_faiss.index"

TEXT_COLUMN = "DataContents_text"
TITLE_COLUMN = "Title"  # ì—†ìœ¼ë©´ ìë™ ë¬´ì‹œë¨

# ì§ˆë¬¸-ë‹µë³€ ê²€ìƒ‰ì— ì í•©í•œ ë‹¤êµ­ì–´ ëª¨ë¸
MODEL_NAME = "intfloat/multilingual-e5-base"

# Chunk ì„¤ì • (ë¬¸ë‹¨ ë‹¨ìœ„ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)
CHUNK_SIZE = 300
CHUNK_OVERLAP = 50

# ------------------------------------------------------------
# 3. í…ìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜
# ------------------------------------------------------------
def chunk_text(text, chunk_size=300, overlap=50):
    chunks = []
    start = 0
    text = text.strip()
    length = len(text)

    while start < length:
        end = start + chunk_size
        chunk = text[start:end].strip()
        if len(chunk) > 30:
            chunks.append(chunk)
        start = end - overlap

    return chunks

# ------------------------------------------------------------
# 4. CSV ë¡œë“œ
# ------------------------------------------------------------
df = pd.read_csv(INPUT_CSV)

if TEXT_COLUMN not in df.columns:
    raise ValueError(
        f"'{TEXT_COLUMN}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}"
    )

# ------------------------------------------------------------
# 5. ë¬¸ì„œ â†’ Chunk + Metadata ìƒì„±
# ------------------------------------------------------------
rows = []
chunk_id = 0

for doc_id, row in tqdm(df.iterrows(), total=len(df)):
    body = str(row[TEXT_COLUMN])
    title = str(row[TITLE_COLUMN]) if TITLE_COLUMN in df.columns else ""

    for idx, chunk in enumerate(chunk_text(body)):
        rows.append({
            "chunk_id": chunk_id,
            "doc_id": doc_id,
            "chunk_index": idx,
            "title": title,
            "text": chunk
        })
        chunk_id += 1

chunk_df = pd.DataFrame(rows)
print(f"[INFO] ìƒì„±ëœ Chunk ìˆ˜: {len(chunk_df)}")

# ------------------------------------------------------------
# 6. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
# ------------------------------------------------------------
model = SentenceTransformer(MODEL_NAME)

# e5 ëª¨ë¸ ê·œì¹™: passage / query prefix í•„ìˆ˜
texts_for_embedding = [
    f"passage: {t}"
    for t in chunk_df["text"].tolist()
]

# ------------------------------------------------------------
# 7. Chunk â†’ Vector ë³€í™˜
# ------------------------------------------------------------
embeddings = model.encode(
    texts_for_embedding,
    batch_size=32,
    show_progress_bar=True,
    normalize_embeddings=True
)

print(f"[INFO] Embedding shape: {embeddings.shape}")

# ------------------------------------------------------------
# 8. ì €ì¥
# ------------------------------------------------------------

# (1) ë©”íƒ€ë°ì´í„° CSV
chunk_df.to_csv(OUTPUT_META_CSV, index=False, encoding="utf-8-sig")

# (2) ë²¡í„° NPY
np.save(OUTPUT_EMB_NPY, embeddings)

# (3) FAISS ì¸ë±ìŠ¤
dim = embeddings.shape[1]
index = faiss.IndexFlatIP(dim)  # cosine similarity
index.add(embeddings)
faiss.write_index(index, OUTPUT_FAISS)

print("[DONE]")
print(f"- META CSV : {OUTPUT_META_CSV}")
print(f"- EMB NPY  : {OUTPUT_EMB_NPY}")
print(f"- FAISS   : {OUTPUT_FAISS}")

# ------------------------------
# Cell 8
# ------------------------------
# ============================================================
# Vector Search: "R&Dì‚¬ì—…" (Google Colab)
# ============================================================


import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# ------------------------------------------------------------
# 1. ê²½ë¡œ ì„¤ì •
# ------------------------------------------------------------
META_CSV = "/content/policy_news_chunks_meta.csv"
EMB_NPY  = "/content/policy_news_chunks_embeddings.npy"
FAISS_IDX = "/content/policy_news_faiss.index"

MODEL_NAME = "intfloat/multilingual-e5-base"

TOP_K = 5   # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜

# ------------------------------------------------------------
# 2. ë°ì´í„° ë¡œë“œ
# ------------------------------------------------------------
chunk_df = pd.read_csv(META_CSV)
embeddings = np.load(EMB_NPY)
index = faiss.read_index(FAISS_IDX)

print(f"[INFO] Chunk ìˆ˜: {len(chunk_df)}")
print(f"[INFO] Embedding shape: {embeddings.shape}")

# ------------------------------------------------------------
# 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
# ------------------------------------------------------------
model = SentenceTransformer(MODEL_NAME)

# ------------------------------------------------------------
# 4. ê²€ìƒ‰ì–´ ì„¤ì •
# ------------------------------------------------------------
query = "R&Dì‚¬ì—…"

# e5 ëª¨ë¸ ê·œì¹™: query prefix í•„ìˆ˜
query_vec = model.encode(
    [f"query: {query}"],
    normalize_embeddings=True
)

# ------------------------------------------------------------
# 5. ë²¡í„° ê²€ìƒ‰
# ------------------------------------------------------------
scores, indices = index.search(query_vec, TOP_K)

# ------------------------------------------------------------
# 6. ê²°ê³¼ ì¶œë ¥
# ------------------------------------------------------------
print(f"\n[QUERY] {query}")
print("-" * 80)

for rank, idx in enumerate(indices[0]):
    row = chunk_df.iloc[idx]
    score = scores[0][rank]

    print(f"\n[TOP {rank+1}]  score = {score:.4f}")
    print(f"ì œëª©      : {row.get('title', '')}")
    print(f"ë¬¸ì„œ ID  : {row['doc_id']}, Chunk ID: {row['chunk_id']}")
    print("ë³¸ë¬¸ ë°œì·Œ:")
    print(row['text'][:500])
